---
title: 'Week 4: Sampling distributions'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Disclaimer: These labs rely heavily on those developed by Mike Whitlock for his BIOL 300 course at UBC <https://www.zoology.ubc.ca/~whitlock/bio300/>.** In some cases I have used his material *verbatim*, in other cases I have heavily modified them.

## Goals

* Understand the sampling distribution of an estimate
* Investigate sampling error
* Calculate standard error of the mean
* Calculate confidence intervals

## Learning the Tools

### Setup

* As in previous labs, install the **dplyr** and **ggplot2** packages if they aren't already installed.
* Install a new package called **sn**

### Creating your own function

*R* has many functions; additional *R* packages like **dplyr** and **ggplot2** add thousands of more function. Nevertheless, it can be useful to write your own function to complete a specific task. Today, we'll write a function to help simulate a sampling distribution. You'll just copy and paste the code, but it will show you how to go about writing your own functions.

You can define a new function at the top of your script, but I prefer to keep all my custom functions associated with a project in their own file. To do that, create a new R script by going to "File > New File > R Script", like you do when create a new file for your lab report.

Next, copy-and-paste the code below into the new file and save the file with the name "functions.R" in your project directory.

```{r}

# Custom function to generate sampling distribution of the sample mean from a numeric vector x with sample size n
sample_mean <- function(x, n, n_sim) {
  
  data.frame(
    sim = gl(n_sim, n),
    Y = sample(x, n * n_sim, replace = TRUE)
  ) %>%
    group_by(sim) %>%
    summarize(Ybar = mean(Y))

}

```

You can name your functions whatever you want, but like variable names, they should be clear, informative, and no longer than necessary. You also need to omit spaces and dashes, so use the `_` instead. Unlike variable names, it's often helpful to call your function name a verb describing the action it performs.

Notice that this function takes three arguments: `x`, `n`, and `n_sim`. `x` will be a numeric vector we want to randomly sample from; `n` is our sample size, so it should be an integer; and `n_sim` is number of simulated samples, so it should also be an integer.

Don't worry too much about the code inside the function. It uses some tricks we haven't gone over yet, but feel free to explore on your own time. It will return a `data.frame` with two columns. The `sim` column will number the simulations from 1 to `n_sim`. The `Ybar` column will report the sample mean ($\bar{Y}$) from the random sample of $n$ observations from `x`.

Once you've copied the `sample_mean()` function into your new R script, save the script as "functions.R".

#### Using `source()` to read *R* code from a file.

Once you've saved the `sample_mean()` function code in "functions.R", that new function is still not available until you execute the code. To do that, use the `source()` function and put the following code near the top of your lab report (below your name, etc.) so that it looks something like this:

```{r, echo = TRUE, eval = FALSE}
# Chris Muir
# Lab Report Week 4

library(dplyr)
library(ggplot2)

source("functions.R")

```

Notice that I've also used the `library()` function to load the **dplyr** and **ggplot2** packages. You should do the same.

```{r, echo = FALSE, eval = TRUE, message=FALSE, warning = FALSE}

library(dplyr)
library(ggplot2)

source("../functions.R")

```

#### Running your new function

Now you should be able to use the `sample_mean()` function. Try this simple example. First, you'll create a "Population" to randomly sample by drawing 100 random numbers from a Normal distribution using the `rnorm()` function. Then, you'll use that vector for the `sample_mean()` function. Your output should look something like mine, but not exactly the same because of random sampling.

```{r}

# Random sample of n = 100 from a Normal distribution with mean of 0 and standard deviation of 1
population <- rnorm(n = 100, mean = 0, sd = 1)
sample_mean(x = population, n = 10, n_sim = 10)

```

On your own, try the `sample_mean()` function with different values of `n` and `n_sim`.

### Generating random numbers

Generating random numbers in *R* can be extremely useful for randomly sampling or generating simulated data to test your statistical methods. In the section above, we used `rnorm()` to randomly sample from a Normal distribution. *R* has lots of similar functions for other probability distributions, e.g. `rbinom()` randomly samples from a Binomial distribution, `rpois()` randomly samples from a Poisson distribution, etc. We'll illustrate this by sampling from skewed distributions to illustrate symmetrical, left-skewed, and right-skewed distributions.

We'll use exciting new functions in the **sn** package for this (don't worry about the details of these distributions.). The code below makes a `data.frame` with a `shape` and a `Y` column. The `shape` column will tell whether `Y` is sampled from a Left Skewed, Not Skewed, or Right Skewed distribution. We used the `rsn(n, alpha = -5)` function to sample from Leaf Skewed Normal distribution,  `rnorm(n)` to sample from an unskewed Normal distribution, and the `rsn(n, alpha = 5)` function to sample from Right Skewed Normal distribution. The `head()` function lets us peak at the first few rows.

```{r, message = FALSE}

library(sn)

n <- 1e4 # Number of random samples
shapes <- c( "Left Skewed", "Not skewed", "Right Skewed")
df <- data.frame(
  shape = rep(shapes, each = n),
  Y = c(rsn(n, alpha = -5), rnorm(n), rsn(n, alpha = 5))
)

head(df)

```

Now let's plot the results using `ggplot()`. We'll introduce a new function `facet_wrap()` that lets us make multiples of the same graphic for different groups. In this case, we'll have seperate facets for each of the `shape` values.

```{r, message = FALSE}

ggplot(df, aes(Y)) +
  facet_wrap(~ shape) +
  geom_histogram() +
  theme_bw()

```

Modify the code above to generate a density plot using the `geom_density()` function. Within `geom_density()`, use the `fill = ...` argument to choose an exciting color. I've chosen tomato. You can see other available colors by running the command `colors()`. Your result should look something like this:

```{r, echo = FALSE, message = FALSE}

ggplot(df, aes(Y)) +
  facet_wrap(~ shape) +
  geom_density(fill = "tomato") +
  theme_bw()

```

### Standard error of the mean

The standard error of the mean ($\mathrm{SE}_{\bar{Y}}$) quantifies our uncertainty in our estimate of the population mean, $\bar{Y}$. Specifically, $\mathrm{SE}_{\bar{Y}}$ is the standard deviation of sampling distribution for $\bar{Y}$. The equation for the $\mathrm{SE}_{\bar{Y}}$ is the sample standard deviation divided by the square-root of the sample size:

$$ \mathrm{SE}_{\bar{Y}} = \frac{s}{\sqrt{n}} $$

There's no function in *R* to calculate $\mathrm{SE}_{\bar{Y}}$, so let's write our own and add it the "functions.R" file. Make a function called `se_mean()` that takes a vector of numbers and returns $\mathrm{SE}_{\bar{Y}}$. Then, copy the code into "functions.R" and save the file. To use the function, you'll need to run the `source("functions.R")` command again. If you get stuck, unfold the code below to reveal the function.

<details>
  <summary>Show code</summary>
  ```{r, eval = FALSE}
  
  se_mean <- function(x) {
    sd(x) / sqrt(length(x))
  }
  
  ```
</details>

```{r, echo = FALSE, eval = TRUE} 

  se_mean <- function(x) {
    sd(x) / sqrt(length(x))
  }
  x <- c(2.16, -0.79, -0.18,  1.62, -0.98, -1.15, -0.15,  1.34,  1.96,  1.74)
  se <- se_mean(x)

```


Now use the function on the dataset below.

> 2.16 -0.79 -0.18  1.62 -0.98 -1.15 -0.15  1.34  1.96  1.74

You should get $\mathrm{SE}_{\bar{Y}}$ = `r round(se, 3)`.

### 95% confidence intervals

Confidence intervals are a way to show the plausible range of parameter values given the data. 95% confidence intervals will include the true population parameter 95% of the time. We'll learn ways to calculuate confidence intervals for different parameters throughout the class. Today, we'll use the "2 SE" rule to approximate 95% confidence intervals for the sample mean $\bar{Y}$. The lower bound and upper bounds of the approximate 95% confidence interval using the 2 SE rule are:

$$ \text{lower CI}: \bar{Y} - 2 \times \mathrm{SE}_{\bar{Y}} $$
$$ \text{upper CI}: \bar{Y} + 2 \times \mathrm{SE}_{\bar{Y}} $$
Use the `mean()` and `se_mean()` functions to calculate the confidence interval for dataset used in the last section. You should get:

```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE} 

  library(kableExtra)
  library(tibble)
  tibble(
    `$\\bar{Y}$` = mean(x),
    `Lower CI` = mean(x) - 2 * se_mean(x),
    `Upper CI` = mean(x) + 2 * se_mean(x)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = FALSE)

```

## Activities

### Distribution of sample means

Go to the web and open the page at http://www.zoology.ubc.ca/~whitlock/kingfisher/SamplingNormal.htm. This page contains some interactive visualizations that let you play around with sampling to see the distribution of sampling means. Click the button that says “Tutorial” near the bottom of the page and follow along with the instructions.

### Confidence intervals

Go back to the web, and open the page at http://www.zoology.ubc.ca/~whitlock/kingfisher/CIMean.htm. This applet draws confidence intervals for the mean of a known population. Click “Tutorial” again, and follow along.

## Questions

### 1. Sampling distribution of $\bar{Y}$

#### a. Import data

As we did in class, we'll use the leaf size data from Wright *et al.* 2017^[The original paper can be found [here](https://doi.org/10.1126/science.aal4760)]. We'll pretend that this is population of all leaf sizes in the world and look at the properties of random samples from the population.

Use the `read.csv()` and `filter()` functions from last week's lab to:

* import the data as a `data.frame()`
* assign the `data.frame()` with the name `leafsize`
* use `filter()` to remove missing values from the `leafsize_cm2` column

If you've done everything correctly, you should get the same values for the population mean seen below:

```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}

leafsize <- read.csv("../Data/wright_etal_2017.csv") %>% 
  filter(!is.na(leafsize_cm2))

```

```{r, message = FALSE, warning = FALSE}

mean(leafsize$leafsize_cm2)

```

#### b. Sampling distributions for $n = 64$, $n = 256$, and $n = 1024$

Now we'll use our new `sample_mean()` function to look at how the sampling distribution of the sample mean changes with $n$, the sample size. First, let's do a small example with $n = 64$ and 10 simulations. Your results should be similar, but not identical because of random sampling.

```{r}

# define our population
population <- leafsize$leafsize_cm2

n <- 64 # sample size
n_sim <- 10 # number of simulations

sample_dist <- sample_mean(x = population, n = n, n_sim = n_sim)

# extract sample means from sample_dist
sample_dist$Ybar

```

Now, increase the number of simulations to 1000 (`n_sim <- 1000`) and try generate the figures below by (heavily) modifying the code we used above on randomly sampling from distributions with different skew. First, try on your own, and if you get stuck, you can unhide my code. A few hints:

* Rather than using `rnorm()`, use `sample_mean()` with different values of $n$
* To create facets in single column rather than a row, use this code `  facet_wrap(~ sample_size, ncol = 1)`

<details>
  <summary>Show code</summary>
  ```{r, eval = FALSE}
  
  n_sim <- 1e3
  sample_sizes <- c("a. n = 64", "b. n = 256", "c. n = 1024")
  sample_dists <- data.frame(
    sample_size = rep(sample_sizes, each = n_sim),
    Ybar = c(
      sample_mean(x = population, n = 64, n_sim = n_sim)$Ybar,
      sample_mean(x = population, n = 256, n_sim = n_sim)$Ybar,
      sample_mean(x = population, n = 1024, n_sim = n_sim)$Ybar
    )
  )

  ggplot(sample_dists, aes(Ybar)) +
    facet_wrap(~ sample_size, ncol = 1) +
    xlab("Sample mean") +
    geom_histogram() +
    theme_bw()
  
  ```
</details>

```{r, echo = FALSE, eval = TRUE, message = FALSE}

  n_sim <- 1e3
  sample_sizes <- c("a. n = 64", "b. n = 256", "c. n = 1024")
  sample_dists <- data.frame(
    sample_size = rep(sample_sizes, each = n_sim),
    Ybar = c(
      sample_mean(x = population, n = 64, n_sim = n_sim)$Ybar,
      sample_mean(x = population, n = 256, n_sim = n_sim)$Ybar,
      sample_mean(x = population, n = 1024, n_sim = n_sim)$Ybar
    )
  )

  ggplot(sample_dists, aes(Ybar)) +
    facet_wrap(~ sample_size, ncol = 1) +
    xlab("Sample mean") +
    geom_histogram() +
    theme_bw()

```

Again, modify the code to make a density plot like this:

```{r, echo = FALSE, eval = TRUE, message = FALSE}

ggplot(sample_dists, aes(Ybar)) +
  facet_wrap(~ sample_size, ncol = 1) +
  xlab("Sample mean") +
  geom_density(fill = "tomato") +
  theme_bw()

```

#### c. How does the location, width, and skew of the sampling distribution for $\bar{Y}$ change as $n$ increases?

### 2. Compare the width of simulated sampling distributions to the equation for $\mathrm{SE}_{\bar{Y}}$

We have assumed that the `leafsize` dataset represents the entire population of leaves, so we can calculate the "true" mean and standard deviation, as we did in class.

```{r, echo = FALSE, eval = TRUE, message=FALSE, warning = FALSE}

nleaves <- nrow(leafsize)
tibble(
  `$\\mu$` = mean(leafsize$leafsize_cm2),
  `$\\sigma$` = sd(leafsize$leafsize_cm2) * ((nleaves - 1) / nleaves)
) %>%
  knitr::kable() %>%
  kable_styling(full_width = FALSE)

```

#### a. Calculate the *population* standard error of the mean for $n = 64$, $n = 256$, and $n = 1024$

Hint: for the population SE, use $\sigma$ from the table above rather than $s$.

#### b. Calculate the standard deviations of the simulated sampling distributions.

I called the `data.frame` with my simulated sampling distribution `sample_dists`. If you call yours something different, you will need to modify the code below accordingly.

We'll use the `group_by()` and `summarize()` functions from **dplyr** to calculate the standard deviation of the sampling distributions for $\bar{Y}$ under different sample sizes. We used these functions earlier in the `sample_means()` function. `group_by()` takes a `data.frame` and creates subsets for each value of a "group". Usually, this would be each level of a categorical variable. In this case, we'll group by `sample_size` to calculate the standard deviation separarely for each value of $n$.

```{r}

grouped_sample_dists <- group_by(sample_dists, sample_size)

```

The `summarize()` function summarizes one or more variables for each group in a grouped data.frame. We'll calculate the standard deviation of simulated $\bar{Y}$ values -- this is a way to estimate $SE_{\bar{Y}}$ using simulations rather than the equations above. You should get something close to what I have below, but it will be different because of random sampling.

```{r}

summarize(grouped_sample_dists, SE_Ybar = sd(Ybar))

```

#### c. Is the simulated SE from 2b. similar to what you calculated using the equation in 2a.?

### 3. Calculate confidence intervals

The file "caffeine.csv" contains data on the amount of caffeine in a 16 oz. cup of coffee obtained from various vendors. For context, doses of caffeine over 25 mg are enough to increase anxiety in some people, and doses over 300 to 360 mg are enough to significantly increase heart rate in most people. A can of Red Bull contains 80mg of caffeine.

#### a. What is the mean amount of caffeine in 16 oz. coffees?

#### b. What is the 95% confidence interval for the mean? (Use the 2 SE rule)

#### c. Plot the frequency distribution of caffeine levels for these data in a histogram. Is the amount of caffeine in a cup of coffee relatively consistent from one vendor to another? What is the standard deviation of caffeine level? What is the coefficient of variation?

#### d. The file "caffeineStarbucks.csv" has data on six 16 oz. cups of Breakfast Blend coffee sampled on six different days from a Starbucks location. Calculate the mean (and the 95% confidence interval for the mean using the 2 SE rule) for these data. Compare these results to the data taken on the broader sample of vendors in the first file. Describe the difference.

### 4. Calculate approximate 99% confidence intervals

A confidence interval is a range of values that are likely to contain the true value of a parameter. Consider the "caffeine.csv" data again.

#### a. Calculate the approximate 99% confidence interval for the mean caffeine level by using 2.5 SE's rather than 2 SE.

#### b. Compare this 99% confidence interval to the 95% confidence interval you calculate in question 3b. Which confidence interval is wider (i.e., spans a broader range)? Why should this one be wider?

#### c. Let’s compare the quantiles of the distribution of caffeine to this confidence interval. Approximately 95% of the data values should fall between the 2.5% and 97.5% quantiles of the distribution of caffeine. (Explain why this is true.) We can use *R* to calculate the 2.5% and 97.5% quantiles with a command like the following. (Replace `datavector` with the name of the vector of your caffeine data.)

```{r, echo = TRUE, eval = FALSE}
quantile(datavector, c(0.025, 0.975), na.rm = TRUE)
```

Are these the same as the boundaries of the 95% confidence interval? If not, why not? Which should bound a smaller region, the quantile or the confidence interval of the mean?
